{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb7d5bf-32d1-4797-8ded-efa5f333d375",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459d59b1-a56f-4aa2-85ab-00243c802ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "df_train = pd.read_csv(\"train_set.csv\", index_col='Original Index')\n",
    "df_valid = pd.read_csv(\"valid_set.csv\", index_col='Original Index')\n",
    "df_test = pd.read_csv(\"test_set.csv\", index_col='Original Index')\n",
    "\n",
    "X_train = df_train[[col for col in df_train.columns if col != 'POI']]\n",
    "y_train = df_train['POI']\n",
    "\n",
    "X_valid = df_valid[[col for col in df_valid.columns if col != 'POI']]\n",
    "y_valid = df_valid['POI']\n",
    "\n",
    "X_test = df_test[[col for col in df_test.columns if col != 'POI']]\n",
    "y_test = df_test['POI']\n",
    "\n",
    "print(f\"Training: {X_train.shape[0]}, Validation: {X_valid.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# get NLTK's stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "# POS tagging\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pos_normalizer(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV   \n",
    "    return wordnet.NOUN\n",
    "  \n",
    "# stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# n-gram generator\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ab0f247-cdf9-417e-aeeb-4714ce1b4d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize, unigram\n",
    "def preprocess_1(s):\n",
    "    # tokenize\n",
    "    tokens = [token for token in word_tokenize(s)]\n",
    "    \n",
    "    # remove numbers and apostrophes and convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens if len(re.findall(\"[“”'’`\\d]+\", token)) == 0]\n",
    "\n",
    "    # assign POS\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    # lemmatize\n",
    "    lemmas = [lemmatizer.lemmatize(token, pos=pos_normalizer(pos)) for token, pos in pos_tags]\n",
    "\n",
    "    # remove stopwords and punctuation\n",
    "    tokens = [token for token in lemmas if (token not in stopwords and token not in string.punctuation)]\n",
    "\n",
    "    # n-gram\n",
    "    n = 1\n",
    "    feats = [\" \".join(gram) for gram in list(ngrams(tokens, n))]\n",
    "    \n",
    "    return feats\n",
    "\n",
    "# stem, unigram\n",
    "def preprocess_2(s):\n",
    "    # tokenize\n",
    "    tokens = [token for token in word_tokenize(s)]\n",
    "    \n",
    "    # remove numbers and apostrophes and convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens if len(re.findall(\"[“”'’`\\d]+\", token)) == 0]\n",
    "\n",
    "    # assign POS\n",
    "    pos_tags = pos_tag(tokens)\n",
    " \n",
    "    # stem\n",
    "    stems = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    # remove stopwords and punctuation\n",
    "    tokens = [token for token in stems if (token not in stopwords and token not in string.punctuation)]\n",
    "\n",
    "    # n-gram\n",
    "    n = 1\n",
    "    feats = [\" \".join(gram) for gram in list(ngrams(tokens, n))]\n",
    "    \n",
    "    return feats\n",
    "\n",
    "# lemmatize, bigram\n",
    "def preprocess_3(s):\n",
    "    # tokenize\n",
    "    tokens = [token for token in word_tokenize(s)]\n",
    "    \n",
    "    # remove numbers and apostrophes and convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens if len(re.findall(\"[“”'’`\\d]+\", token)) == 0]\n",
    "\n",
    "    # assign POS\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    # lemmatize\n",
    "    lemmas = [lemmatizer.lemmatize(token, pos=pos_normalizer(pos)) for token, pos in pos_tags]\n",
    "\n",
    "    # remove stopwords and punctuation\n",
    "    tokens = [token for token in lemmas if (token not in stopwords and token not in string.punctuation)]\n",
    "\n",
    "    # n-gram\n",
    "    n = 2\n",
    "    feats = [\" \".join(gram) for gram in list(ngrams(tokens, n))]\n",
    "    \n",
    "    return feats\n",
    "\n",
    "# stem, bigram\n",
    "def preprocess_4(s):\n",
    "    # tokenize\n",
    "    tokens = [token for token in word_tokenize(s)]\n",
    "    \n",
    "    # remove numbers and apostrophes and convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens if len(re.findall(\"[“”'’`\\d]+\", token)) == 0]\n",
    "\n",
    "    # assign POS\n",
    "    pos_tags = pos_tag(tokens)\n",
    " \n",
    "    # stem\n",
    "    stems = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    # remove stopwords and punctuation\n",
    "    tokens = [token for token in stems if (token not in stopwords and token not in string.punctuation)]\n",
    "\n",
    "    # n-gram\n",
    "    n = 2\n",
    "    feats = [\" \".join(gram) for gram in list(ngrams(tokens, n))]\n",
    "    \n",
    "    return feats\n",
    "\n",
    "# # lemmatize, unigram, no stopword removal\n",
    "# def preprocess_5(s):\n",
    "#     # tokenize\n",
    "#     tokens = [token for token in word_tokenize(s)]\n",
    "    \n",
    "#     # remove numbers and apostrophes and convert to lowercase\n",
    "#     tokens = [token.lower() for token in tokens if len(re.findall(\"[“”'’`\\d]+\", token)) == 0]\n",
    "\n",
    "#     # assign POS\n",
    "#     pos_tags = pos_tag(tokens)\n",
    "\n",
    "#     # lemmatize\n",
    "#     lemmas = [lemmatizer.lemmatize(token, pos=pos_normalizer(pos)) for token, pos in pos_tags]\n",
    "\n",
    "#     # remove punctuation\n",
    "#     tokens = [token for token in lemmas if (token not in string.punctuation)]\n",
    "\n",
    "#     # n-gram\n",
    "#     n = 1\n",
    "#     feats = [\" \".join(gram) for gram in list(ngrams(tokens, n))]\n",
    "    \n",
    "#     return feats\n",
    "\n",
    "# # stem, unigram, no stopword removal\n",
    "# def preprocess_6(s):\n",
    "#     # tokenize\n",
    "#     tokens = [token for token in word_tokenize(s)]\n",
    "    \n",
    "#     # remove numbers and apostrophes and convert to lowercase\n",
    "#     tokens = [token.lower() for token in tokens if len(re.findall(\"[“”'’`\\d]+\", token)) == 0]\n",
    "\n",
    "#     # assign POS\n",
    "#     pos_tags = pos_tag(tokens)\n",
    " \n",
    "#     # stem\n",
    "#     stems = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "#     # remove punctuation\n",
    "#     tokens = [token for token in stems if (token not in string.punctuation)]\n",
    "\n",
    "#     # n-gram\n",
    "#     n = 1\n",
    "#     feats = [\" \".join(gram) for gram in list(ngrams(tokens, n))]\n",
    "    \n",
    "#     return feats\n",
    "\n",
    "# # lemmatize, bigram, no stopword removal\n",
    "# def preprocess_7(s):\n",
    "#     # tokenize\n",
    "#     tokens = [token for token in word_tokenize(s)]\n",
    "    \n",
    "#     # remove numbers and apostrophes and convert to lowercase\n",
    "#     tokens = [token.lower() for token in tokens if len(re.findall(\"[“”'’`\\d]+\", token)) == 0]\n",
    "\n",
    "#     # assign POS\n",
    "#     pos_tags = pos_tag(tokens)\n",
    "\n",
    "#     # lemmatize\n",
    "#     lemmas = [lemmatizer.lemmatize(token, pos=pos_normalizer(pos)) for token, pos in pos_tags]\n",
    "\n",
    "#     # remove punctuation\n",
    "#     tokens = [token for token in lemmas if (token not in string.punctuation)]\n",
    "\n",
    "#     # n-gram\n",
    "#     n = 2\n",
    "#     feats = [\" \".join(gram) for gram in list(ngrams(tokens, n))]\n",
    "    \n",
    "#     return feats\n",
    "\n",
    "# # stem, bigram, no stopword removal\n",
    "# def preprocess_8(s):\n",
    "#     # tokenize\n",
    "#     tokens = [token for token in word_tokenize(s)]\n",
    "    \n",
    "#     # remove numbers and apostrophes and convert to lowercase\n",
    "#     tokens = [token.lower() for token in tokens if len(re.findall(\"[“”'’`\\d]+\", token)) == 0]\n",
    "\n",
    "#     # assign POS\n",
    "#     pos_tags = pos_tag(tokens)\n",
    " \n",
    "#     # stem\n",
    "#     stems = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "#     # remove punctuation\n",
    "#     tokens = [token for token in stems if (token not in string.punctuation)]\n",
    "\n",
    "#     # n-gram\n",
    "#     n = 2\n",
    "#     feats = [\" \".join(gram) for gram in list(ngrams(tokens, n))]\n",
    "    \n",
    "#     return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dabc803d-e24d-4ca6-b0ab-df340d4c2f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "472bd9c6-461b-4f74-a23e-935d2a9052a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, preprocess_func=preprocess_1, model_type=\"naive\", hyperparam=1):\n",
    "    \n",
    "    # preprocess the training set\n",
    "    X_processed = [preprocess_func(x) for x in X_train]\n",
    "    \n",
    "    # initialize vectorizer\n",
    "    vectorizer = TfidfVectorizer(analyzer=(lambda x: x))\n",
    "        \n",
    "    # fit vectorizer\n",
    "    X = vectorizer.fit_transform(X_processed)\n",
    "    \n",
    "    # initialize the classifier\n",
    "    if model_type == \"naive\":\n",
    "        # clf = MultinomialNB(alpha=hyperparam)\n",
    "        clf = ComplementNB(alpha=hyperparam)\n",
    "    elif model_type == \"svm\":\n",
    "        clf = SVC(C=hyperparam)\n",
    "    elif model_type == \"logistic\":\n",
    "        clf = LogisticRegression(C=hyperparam)\n",
    "    \n",
    "    # fit the model\n",
    "    clf.fit(X, y_train)\n",
    "    \n",
    "    return clf, vectorizer\n",
    "\n",
    "def get_preds(X, clf, vectorizer, preprocess_func):\n",
    "    X_processed = [preprocess_func(x) for x in X]\n",
    "    X_vectorized = vectorizer.transform(X_processed)\n",
    "    return clf.predict(X_vectorized)\n",
    "\n",
    "def compute_f1(X, y_true, clf, vectorizer, preprocess_func):\n",
    "    y_pred = get_preds(X, clf, vectorizer, preprocess_func)\n",
    "    return precision_recall_fscore_support(y_true, y_pred)[2]\n",
    "\n",
    "def compute_acc(X, y_true, clf, vectorizer, preprocess_func):\n",
    "    X_processed = [preprocess_func(x) for x in X]\n",
    "    X_vectorized = vectorizer.transform(X_processed)\n",
    "    return clf.score(X_vectorized, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "547e4611-e99d-497c-97de-e661b7926609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor: Lemmatize + Unigram\n",
      "Model: naive\n",
      "alpha = 0.5\n",
      "\tValidation f1: 37.3\n",
      "\tValidation accuracy: 91.7\n",
      "alpha = 1\n",
      "\tValidation f1: 17.5\n",
      "\tValidation accuracy: 91.4\n",
      "alpha = 10\n",
      "\tValidation f1: 1.0999999999999999\n",
      "\tValidation accuracy: 91.10000000000001\n",
      "\n",
      "Model: logistic\n",
      "C = 0.5\n",
      "\tValidation f1: 39.0\n",
      "\tValidation accuracy: 93.60000000000001\n",
      "C = 1\n",
      "\tValidation f1: 55.1\n",
      "\tValidation accuracy: 94.69999999999999\n",
      "C = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation f1: 74.7\n",
      "\tValidation accuracy: 96.39999999999999\n",
      "\n",
      "\n",
      "Preprocessor: Stem + Unigram\n",
      "Model: naive\n",
      "alpha = 0.5\n",
      "\tValidation f1: 40.6\n",
      "\tValidation accuracy: 91.7\n",
      "alpha = 1\n",
      "\tValidation f1: 17.1\n",
      "\tValidation accuracy: 91.2\n",
      "alpha = 10\n",
      "\tValidation f1: 0.0\n",
      "\tValidation accuracy: 91.10000000000001\n",
      "\n",
      "Model: logistic\n",
      "C = 0.5\n",
      "\tValidation f1: 40.699999999999996\n",
      "\tValidation accuracy: 93.60000000000001\n",
      "C = 1\n",
      "\tValidation f1: 55.800000000000004\n",
      "\tValidation accuracy: 94.69999999999999\n",
      "C = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation f1: 74.9\n",
      "\tValidation accuracy: 96.39999999999999\n",
      "\n",
      "\n",
      "Preprocessor: Lemmatize + Bigram\n",
      "Model: naive\n",
      "alpha = 0.5\n",
      "\tValidation f1: 47.0\n",
      "\tValidation accuracy: 91.9\n",
      "alpha = 1\n",
      "\tValidation f1: 43.5\n",
      "\tValidation accuracy: 91.9\n",
      "alpha = 10\n",
      "\tValidation f1: 41.9\n",
      "\tValidation accuracy: 92.0\n",
      "\n",
      "Model: logistic\n",
      "C = 0.5\n",
      "\tValidation f1: 43.9\n",
      "\tValidation accuracy: 94.0\n",
      "C = 1\n",
      "\tValidation f1: 53.6\n",
      "\tValidation accuracy: 94.5\n",
      "C = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation f1: 67.7\n",
      "\tValidation accuracy: 95.7\n",
      "\n",
      "\n",
      "Preprocessor: Stem + Bigram\n",
      "Model: naive\n",
      "alpha = 0.5\n",
      "\tValidation f1: 44.3\n",
      "\tValidation accuracy: 91.60000000000001\n",
      "alpha = 1\n",
      "\tValidation f1: 38.7\n",
      "\tValidation accuracy: 91.5\n",
      "alpha = 10\n",
      "\tValidation f1: 40.2\n",
      "\tValidation accuracy: 92.0\n",
      "\n",
      "Model: logistic\n",
      "C = 0.5\n",
      "\tValidation f1: 40.2\n",
      "\tValidation accuracy: 93.7\n",
      "C = 1\n",
      "\tValidation f1: 50.9\n",
      "\tValidation accuracy: 94.39999999999999\n",
      "C = 10\n",
      "\tValidation f1: 67.7\n",
      "\tValidation accuracy: 95.7\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "email_col = 'Classify Email'\n",
    "\n",
    "preprocessors = [preprocess_1, preprocess_2, preprocess_3, preprocess_4]\n",
    "model_types = [\"naive\", \"logistic\"]\n",
    "hyperparams = [0.5, 1, 10]\n",
    "# model_types = [\"logistic\"]\n",
    "# hyperparams = [10, 20, 50, 100]\n",
    "\n",
    "preprocessing_dict = {\n",
    "    preprocess_1 : \"Lemmatize + Unigram\",\n",
    "    preprocess_2 : \"Stem + Unigram\",\n",
    "    preprocess_3 : \"Lemmatize + Bigram\",\n",
    "    preprocess_4 : \"Stem + Bigram\"\n",
    "}\n",
    "\n",
    "best_model = {\n",
    "    \"preprocessor\": preprocess_1,\n",
    "    \"model\": \"naive\",\n",
    "    \"hyperparam\": 0.5,\n",
    "    \"valid_acc\": 0\n",
    "}\n",
    "\n",
    "for preprocessor in preprocessors:\n",
    "    print(f\"Preprocessor: {preprocessing_dict[preprocessor]}\")\n",
    "    for model_type in model_types:\n",
    "        print(f\"Model: {model_type}\")\n",
    "        for hyperparam in hyperparams:\n",
    "            if model_type == \"naive\":\n",
    "                print(f\"alpha = {hyperparam}\")\n",
    "            else:\n",
    "                print(f\"C = {hyperparam}\")\n",
    "                \n",
    "            clf, vectorizer = train_model(X_train[email_col], y_train, preprocess_func=preprocessor, model_type=model_type, hyperparam=hyperparam)\n",
    "            \n",
    "            valid_f1 = compute_f1(X_valid[email_col], y_valid, clf, vectorizer, preprocessor)\n",
    "            valid_acc = compute_acc(X_valid[email_col], y_valid, clf, vectorizer, preprocessor)\n",
    "\n",
    "            print(f\"\\tValidation f1: {np.round(valid_f1[1], 3)*100}\") \n",
    "            print(f\"\\tValidation accuracy: {np.round(valid_acc, 3)*100}\") \n",
    "            \n",
    "            # save model, if it's the best\n",
    "            if valid_acc > best_model['valid_acc']:\n",
    "                best_model[\"preprocessor\"] = preprocessor\n",
    "                best_model[\"model\"] = model_type\n",
    "                best_model[\"hyperparam\"] = hyperparam\n",
    "                best_model[\"valid_acc\"] = valid_acc\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "837f29cd-b659-4101-9340-516fa4bd5c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model:\n",
      "\tPreprocessing: Lemmatize + Unigram\n",
      "\tModel: logistic\n",
      "\tHyperparameter: C = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test f1: 0.7111111111111111\n",
      "Final test accuracy: 0.9607250755287009\n"
     ]
    }
   ],
   "source": [
    "print(\"Best model:\")\n",
    "print(f\"\\tPreprocessing: {preprocessing_dict[best_model['preprocessor']]}\")\n",
    "print(f\"\\tModel: {best_model['model']}\")\n",
    "if best_model['model'] == 'naive':\n",
    "    print(f\"\\tHyperparameter: alpha = {best_model['hyperparam']}\")\n",
    "else:\n",
    "    print(f\"\\tHyperparameter: C = {best_model['hyperparam']}\")\n",
    "\n",
    "clf, vectorizer = train_model(X_train[email_col], y_train, preprocess_func=best_model[\"preprocessor\"], model_type=best_model[\"model\"], hyperparam=best_model[\"hyperparam\"])\n",
    "test_f1 = compute_f1(X_test[email_col], y_test, clf, vectorizer, best_model['preprocessor'])\n",
    "test_acc = compute_acc(X_test[email_col], y_test, clf, vectorizer, best_model['preprocessor'])\n",
    "print(f\"Final test f1: {test_f1[1]}\")\n",
    "print(f\"Final test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e31c9f6f-88f0-408d-87ab-7b1e990956ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_func = best_model[\"preprocessor\"]\n",
    "# model_type = best_model[\"model\"]\n",
    "# hyperparam = best_model[\"hyperparam\"]\n",
    "\n",
    "# # preprocess the training set\n",
    "# X_processed = [preprocess_func(x) for x in X_train[email_col]]\n",
    "\n",
    "# # initialize vectorizer\n",
    "# vectorizer = TfidfVectorizer(analyzer=(lambda x: x))\n",
    "\n",
    "# # fit vectorizer\n",
    "# X = vectorizer.fit_transform(X_processed)\n",
    "\n",
    "# # initialize the classifier\n",
    "# if model_type == \"naive\":\n",
    "#     # clf = MultinomialNB(alpha=hyperparam)\n",
    "#     clf = ComplementNB(alpha=hyperparam)\n",
    "# elif model_type == \"svm\":\n",
    "#     clf = SVC(C=hyperparam)\n",
    "# elif model_type == \"logistic\":\n",
    "#     clf = LogisticRegression(C=hyperparam)\n",
    "\n",
    "# # fit the model\n",
    "# clf.fit(X, y_train)\n",
    "\n",
    "# # evaluate on the validation set\n",
    "# X_valid_processed = [preprocess_func(x) for x in X_valid[email_col]]\n",
    "# X_v = vectorizer.transform(X_valid_processed)\n",
    "\n",
    "# # evaluate on the test set\n",
    "# X_test_processed = [preprocess_func(x) for x in X_test[email_col]]\n",
    "# X_t = vectorizer.transform(X_test_processed)\n",
    "\n",
    "# y_pred = clf.predict(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa48319c-bd2b-4b0e-a547-f498addb9440",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = get_preds(X_test[email_col], clf, vectorizer, best_model['preprocessor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ec4f1588-8981-4f36-b4fa-76b81ffe6629",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_vocab = {vectorizer.vocabulary_[key]:key for key in vectorizer.vocabulary_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d67d6e39-b8ea-41a9-a196-ad956cca31ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.DataFrame(clf.coef_.reshape((-1,)), columns=[\"Parameter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0b146e34-740c-4054-a209-4e9a4d13e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs['Parameter Positive'] = coefs['Parameter'].apply(lambda x: 0 if x < 0 else 1)\n",
    "coefs[\"|Parameter|\"] = np.abs(coefs['Parameter'])\n",
    "\n",
    "coefs['Word'] = [inv_vocab[key] for key in coefs.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d1da2816-68e5-41c3-bb54-7cedce9cb1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Parameter Positive</th>\n",
       "      <th>|Parameter|</th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9647</th>\n",
       "      <td>17.380396</td>\n",
       "      <td>1</td>\n",
       "      <td>17.380396</td>\n",
       "      <td>regard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3612</th>\n",
       "      <td>8.750445</td>\n",
       "      <td>1</td>\n",
       "      <td>8.750445</td>\n",
       "      <td>dont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>6.352372</td>\n",
       "      <td>1</td>\n",
       "      <td>6.352372</td>\n",
       "      <td>.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11817</th>\n",
       "      <td>6.118327</td>\n",
       "      <td>1</td>\n",
       "      <td>6.118327</td>\n",
       "      <td>thxs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5682</th>\n",
       "      <td>6.086401</td>\n",
       "      <td>1</td>\n",
       "      <td>6.086401</td>\n",
       "      <td>however</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8787</th>\n",
       "      <td>-6.073085</td>\n",
       "      <td>0</td>\n",
       "      <td>6.073085</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4382</th>\n",
       "      <td>5.804222</td>\n",
       "      <td>1</td>\n",
       "      <td>5.804222</td>\n",
       "      <td>expense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3589</th>\n",
       "      <td>5.524953</td>\n",
       "      <td>1</td>\n",
       "      <td>5.524953</td>\n",
       "      <td>doesnt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3935</th>\n",
       "      <td>-5.498056</td>\n",
       "      <td>0</td>\n",
       "      <td>5.498056</td>\n",
       "      <td>email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4060</th>\n",
       "      <td>5.437156</td>\n",
       "      <td>1</td>\n",
       "      <td>5.437156</td>\n",
       "      <td>ensure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4528</th>\n",
       "      <td>-5.227913</td>\n",
       "      <td>0</td>\n",
       "      <td>5.227913</td>\n",
       "      <td>fax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5977</th>\n",
       "      <td>-5.157084</td>\n",
       "      <td>0</td>\n",
       "      <td>5.157084</td>\n",
       "      <td>information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10841</th>\n",
       "      <td>5.104677</td>\n",
       "      <td>1</td>\n",
       "      <td>5.104677</td>\n",
       "      <td>specific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4293</th>\n",
       "      <td>-5.081165</td>\n",
       "      <td>0</td>\n",
       "      <td>5.081165</td>\n",
       "      <td>ex-associate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8845</th>\n",
       "      <td>4.910828</td>\n",
       "      <td>1</td>\n",
       "      <td>4.910828</td>\n",
       "      <td>portal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3393</th>\n",
       "      <td>4.904681</td>\n",
       "      <td>1</td>\n",
       "      <td>4.904681</td>\n",
       "      <td>didnt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291</th>\n",
       "      <td>4.889075</td>\n",
       "      <td>1</td>\n",
       "      <td>4.889075</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>4.848354</td>\n",
       "      <td>1</td>\n",
       "      <td>4.848354</td>\n",
       "      <td>attached</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11237</th>\n",
       "      <td>4.646298</td>\n",
       "      <td>1</td>\n",
       "      <td>4.646298</td>\n",
       "      <td>strategy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5780</th>\n",
       "      <td>4.573450</td>\n",
       "      <td>1</td>\n",
       "      <td>4.573450</td>\n",
       "      <td>ie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Parameter  Parameter Positive  |Parameter|          Word\n",
       "9647   17.380396                   1    17.380396        regard\n",
       "3612    8.750445                   1     8.750445          dont\n",
       "52      6.352372                   1     6.352372         .....\n",
       "11817   6.118327                   1     6.118327          thxs\n",
       "5682    6.086401                   1     6.086401       however\n",
       "8787   -6.073085                   0     6.073085            pm\n",
       "4382    5.804222                   1     5.804222       expense\n",
       "3589    5.524953                   1     5.524953        doesnt\n",
       "3935   -5.498056                   0     5.498056         email\n",
       "4060    5.437156                   1     5.437156        ensure\n",
       "4528   -5.227913                   0     5.227913           fax\n",
       "5977   -5.157084                   0     5.157084   information\n",
       "10841   5.104677                   1     5.104677      specific\n",
       "4293   -5.081165                   0     5.081165  ex-associate\n",
       "8845    4.910828                   1     4.910828        portal\n",
       "3393    4.904681                   1     4.904681         didnt\n",
       "2291    4.889075                   1     4.889075        coffee\n",
       "1022    4.848354                   1     4.848354      attached\n",
       "11237   4.646298                   1     4.646298      strategy\n",
       "5780    4.573450                   1     4.573450            ie"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs.sort_values('|Parameter|', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bd907c61-61bc-46c5-9102-36bc07d35416",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec, rec, f1, supp = precision_recall_fscore_support(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e78de4e-7020-42e5-9c20-5f8fd7d17816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:\n",
      "\tNon-POI = 0.9642\n",
      "\tPOI = 0.8989\n",
      "Recall:\n",
      "\tNon-POI = 0.9941\n",
      "\tPOI = 0.5882\n",
      "F1:\n",
      "\tNon-POI = 0.9789\n",
      "\tPOI = 0.7111\n",
      "Support:\n",
      "\tNon-POI = 3038\n",
      "\tPOI = 272\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision:\\n\\tNon-POI = {np.round(prec[0], 4)}\\n\\tPOI = {np.round(prec[1], 4)}\")\n",
    "print(f\"Recall:\\n\\tNon-POI = {np.round(rec[0], 4)}\\n\\tPOI = {np.round(rec[1], 4)}\")\n",
    "print(f\"F1:\\n\\tNon-POI = {np.round(f1[0], 4)}\\n\\tPOI = {np.round(f1[1], 4)}\")\n",
    "print(f\"Support:\\n\\tNon-POI = {np.round(supp[0], 4)}\\n\\tPOI = {np.round(supp[1], 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b27626b4-3c01-4569-9cd4-c87932e1155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lots of false negatives -> we're missing a lot of positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d99043bd-d626-4a3a-bfcc-ad45ad0dd704",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7383dd25-7606-4a0d-92e0-31bc832141d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poi_idx = y_test\n",
    "# exec_idx = (X_test['Exec 300']) & (~y_test)\n",
    "# norm_idx = (~X_test['Exec 300']) & (~y_test)\n",
    "\n",
    "# y_pred_poi = get_preds(X_test.loc[poi_idx, email_col], clf, vectorizer, best_model['preprocessor'])\n",
    "# y_pred_exec = get_preds(X_test.loc[exec_idx, email_col], clf, vectorizer, best_model['preprocessor'])\n",
    "# y_pred_norm = get_preds(X_test.loc[norm_idx, email_col], clf, vectorizer, best_model['preprocessor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9e7e73f5-995e-45d9-a7f9-78fb5a3edf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true_poi = y_test[poi_idx]\n",
    "# y_true_exec = y_test[exec_idx]\n",
    "# y_true_norm = y_test[norm_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e41390d4-e13a-4492-bfa9-dffe7301e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = {\n",
    "#     \"POI\": (y_true_poi, y_pred_poi), \n",
    "#     \"Exec\": (y_true_exec, y_pred_exec), \n",
    "#     \"Normal\": (y_true_norm, y_pred_norm) \n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "85ff0a87-26a0-426f-ab4d-d233b1a5b1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POI\n",
      "Precision:\n",
      "\tNon-POI = 0.0\n",
      "\tPOI = 1.0\n",
      "Recall:\n",
      "\tNon-POI = 0.0\n",
      "\tPOI = 0.5882\n",
      "F1:\n",
      "\tNon-POI = 0.0\n",
      "\tPOI = 0.7407\n",
      "Support:\n",
      "\tNon-POI = 0\n",
      "\tPOI = 272\n",
      "\n",
      "Exec\n",
      "Precision:\n",
      "\tNon-POI = 1.0\n",
      "\tPOI = 0.0\n",
      "Recall:\n",
      "\tNon-POI = 0.989\n",
      "\tPOI = 0.0\n",
      "F1:\n",
      "\tNon-POI = 0.9945\n",
      "\tPOI = 0.0\n",
      "Support:\n",
      "\tNon-POI = 363\n",
      "\tPOI = 0\n",
      "\n",
      "Normal\n",
      "Precision:\n",
      "\tNon-POI = 1.0\n",
      "\tPOI = 0.0\n",
      "Recall:\n",
      "\tNon-POI = 0.9948\n",
      "\tPOI = 0.0\n",
      "F1:\n",
      "\tNon-POI = 0.9974\n",
      "\tPOI = 0.0\n",
      "Support:\n",
      "\tNon-POI = 2675\n",
      "\tPOI = 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# for type_ in k:\n",
    "#     print(type_)\n",
    "#     y_t, y_p = k[type_]\n",
    "#     prec, rec, f1, supp = precision_recall_fscore_support(y_t, y_p)\n",
    "    \n",
    "#     print(f\"Precision:\\n\\tNon-POI = {np.round(prec[0], 4)}\\n\\tPOI = {np.round(prec[1], 4)}\")\n",
    "#     print(f\"Recall:\\n\\tNon-POI = {np.round(rec[0], 4)}\\n\\tPOI = {np.round(rec[1], 4)}\")\n",
    "#     print(f\"F1:\\n\\tNon-POI = {np.round(f1[0], 4)}\\n\\tPOI = {np.round(f1[1], 4)}\")\n",
    "#     print(f\"Support:\\n\\tNon-POI = {np.round(supp[0], 4)}\\n\\tPOI = {np.round(supp[1], 4)}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1de3ea-ba7c-4a33-96f2-a1e84c04281e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
